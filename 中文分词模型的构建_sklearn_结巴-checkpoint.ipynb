{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本案例介绍了中文分词的相关知识，并用Python的中文分词库jieba和Genius对文本进行分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "[1. 数据准备](#1)  \n",
    "[2. 中文分词模型](#2)  \n",
    "&emsp;&emsp;[2.1 中文分词](#2.1)  \n",
    "&emsp;&emsp;[2.2 分词技术](#2.2)  \n",
    "[3. 第三方库实现](#3)  \n",
    "&emsp;&emsp;[3.1 jieba](#3.1)  \n",
    "&emsp;&emsp;[3.2 Genius](#3.2)  \n",
    "[4. 参考资料](#4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "\n",
    "import genius\n",
    "print (\"jieba version : \", jieba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 <a id=1></a>数据准备\n",
    "\n",
    "将案例中提到的中文文本保存在变量`word_text`中，并导入中文分词第三方库jieba和Genius。jieba基于前缀词典，从句子中构建词图，生成有所有可能的词所构成的有向无环图；对于未登录词，采用隐马尔科夫模型进行识别。而Genius则是采用条件随机场算法进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加强专业人才培养。创新人才培养模式，建立健全多层次、多类型的大数据人才培养体系。鼓励高校设立数据科学和数据工程相关专业，重点培养专业化数据工程师等大数据专业人才。\n",
      "鼓励采取跨校联合培养等方式开展跨学科大数据综合型人才培养，大力培养具有统计分析、计算机技术、经济管理等多学科知识的跨界复合型人才。鼓励高等院校、职业院校和企业合作，加强职业技能人才实践培养，积极培育大数据技术和应用创新型人才。\n"
     ]
    }
   ],
   "source": [
    "word_text = u'''加强专业人才培养。创新人才培养模式，建立健全多层次、多类型的大数据人才培养体系。鼓励高校设立数据科学和数据工程相关专业，重点培养专业化数据工程师等大数据专业人才。\n",
    "鼓励采取跨校联合培养等方式开展跨学科大数据综合型人才培养，大力培养具有统计分析、计算机技术、经济管理等多学科知识的跨界复合型人才。鼓励高等院校、职业院校和企业合作，加强职业技能人才实践培养，积极培育大数据技术和应用创新型人才。'''\n",
    "\n",
    "print (word_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 <a id=2></a>中文分词模型\n",
    "\n",
    "## 2.1 <a id=2.1></a>中文分词\n",
    "\n",
    "本案例将结合文本描述如何对中文文本进行分词。而对于奇异词和未登录词的识别是中文分词重点关注的问题。对于模型而言，未登录词就是未知的新词。判断一组汉字是否应作为一个词整体出现，需要结合上下文的语境，以及其他方面的知识，模型算法无法做到完全自动化，不需要人为干预。目前，比较成熟的未登录词识别技术包括常见的中文人名识别等。\n",
    "\n",
    "在具体分词的过程中，未登录词通常在文本经过第一次分词模型处理后，作为两两相邻的词组出现。事实上，未登录词的识别过程就是寻找相邻但实际不需要切分的词组, 然后将它们连接起来作为整体的词出现。\n",
    "\n",
    "从另外一个角度来看，中文分词实质上是一个标注文本中每个字符的过程。因此，我们可以将字符的标签分为四类：\n",
    "\n",
    "|B|M|E|S|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|词的开始|词的中间|词的结束|单个字符(汉字)的词|\n",
    "\n",
    "举例来说，对句子`我爱北京天安门`进行分词后的结果为`我/爱/北京/天安门`。根据字符的标签，我们还可以将分词结果表达为`我S爱S北B京E天B安M门E`\n",
    "那么对这句话来说，也就是说，我们要通过“你现在应该去幼儿园了”这个观察序列来确定状态序列，也就是HMM的“解码问题”。比如这句话的解码结果就是：“SBEBESBMES”。\n",
    "\n",
    "如果我们把字符的标签看作字符所在的状态，字符为可观察的字符序列，那么我们可以通过观察字符序列来去确定字符的状态序列。因此，我们可以使用隐马尔科夫模型和条件随机场模型来记性中文分词，也就是序列标注。\n",
    "\n",
    "## 2.2 <a id=2.2></a>分词技术\n",
    "\n",
    "+ **1)隐马尔科夫模型**\n",
    "\n",
    "隐马尔科夫模型将分词的结果看作是模型隐含状态的序列，通过观察字符序列来确定隐含状态序列，就是隐马尔科夫模型的`解码问题`。\n",
    "\n",
    "\n",
    "回顾一下，隐马尔可夫模型$model = (A, B, \\omega)$的主要参数：\n",
    "\n",
    "+ 状态值集合 $Q$\n",
    "\n",
    "+ 观测值集合 $O$\n",
    "\n",
    "+ 转移概率矩阵 $A$\n",
    "\n",
    "+ 发射概率矩阵 $B$\n",
    "\n",
    "+ 初始状态分布 $\\Omega$\n",
    "\n",
    "解码（decoding）问题的具体形式为：\n",
    "\n",
    "已知模型$model$与观测值序列$M$，求解使得条件概率$P(N|M)$最大化的状态序列$N$。 \n",
    "\n",
    "\n",
    "+ **2)条件随机场**\n",
    "\n",
    "隐马尓可夫模型在计算条件概率$P(N|M)$的前提假设为特征独立性，这会让模型无法考虑上下文和语境的信息。条件随机场则很好的地决了这一问题。\n",
    "\n",
    "条件随机场模型仍然把中文分词看作字符标注问题。使用条件随机场模型进行中文分词的过程就是将句子中的字符标注后，把标签`B`和标签`E`之间的字，以及标注为`S`的单个字符作为一个整体的分词。\n",
    "\n",
    "条件随机场(Conditional Random Field)在自然语言处理中的应用集中在以下场景：\n",
    "\n",
    "+ 分词（切分完整的句子，将其转变为词的集合）\n",
    "\n",
    "+ 词性标注（标注分词的词性，比如说动词，名词等）\n",
    "\n",
    "+ 命名实体识别（识别人名，地名等具有一定规律的词）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3 <a id=3></a>第三方库实现\n",
    "\n",
    "## 3.1<a id=3.1></a> jieba\n",
    "\n",
    "jieba分词库会主要使用函数`cut()`来实现中文分词的过程。`cut()`函数的主要输入参数为将要进行分词的文本，其中`cut_all`表明是否在全局模式或者精确模式之间切换。\n",
    "\n",
    "在`cut()`函数的内部，再次调用`__cut()`函数，实现隐马尔科夫模型。更进一步来说，`__cut()`函数会调用`viterbi`算法求出输入文本对应的隐含状态序列，然后基于隐含状态序列分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式：  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.028 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加强/ 专业/ 专业人才/ 人才/ 人才培养/ 培养/ / / 创新/ 新人/ 人才/ 人才培养/ 培养/ 模式/ / / 建立/ 建立健全/ 健全/ 多层/ 多层次/ 层次/ / / 多类/ 类型/ 的/ 大数/ 数据/ 人才/ 人才培养/ 培养/ 体系/ / / 鼓励/ 高校/ 设立/ 数据/ 科学/ 和/ 数据/ 工程/ 相关/ 专业/ / / 重点/ 培养/ 专业/ 专业化/ 数据/ 工程/ 工程师/ 等/ 大数/ 数据/ 专业/ 专业人才/ 人才/ / \n",
      "/ 鼓励/ 采取/ 跨/ 校/ 联合/ 培养/ 等/ 方式/ 开展/ 跨学科/ 学科/ 科大/ 大数/ 数据/ 综合/ 综合型/ 人才/ 人才培养/ 培养/ / / 大力/ 培养/ 具有/ 统计/ 统计分析/ 计分/ 分析/ / / 计算/ 计算机/ 计算机技术/ 算机/ 技术/ / / 经济/ 管理/ 等/ 多/ 学科/ 学科知识/ 知识/ 的/ 跨/ 界/ 复合/ 复合型/ 人才/ / / 鼓励/ 高等/ 高等院校/ 院校/ / / 职业/ 职业院校/ 院校/ 和/ 企业/ 合作/ / / 加强/ 职业/ 职业技能/ 技能/ 能人/ 人才/ 实践/ 培养/ / / 积极/ 培育/ 大数/ 数据/ 技术/ 和/ 应用/ 创新/ 创新型/ 新型/ 人才/ / \n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(word_text, cut_all=True)\n",
    "\n",
    "## 全模式\n",
    "print (u\"全模式：  \\n\")\n",
    "print (\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在全模式下，`cut()`函数返回了所有可能的分词组合，比如`人才/ 人才培养/ 培养/ 体系/`。当然，我们还可以切换为精确模式，给出模型认为最有的分词组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确模式：   \n",
      "\n",
      "加强/ 专业/ 人才培养/ 。/ 创新/ 人才培养/ 模式/ ，/ 建立健全/ 多层次/ 、/ 多/ 类型/ 的/ 大/ 数据/ 人才培养/ 体系/ 。/ 鼓励/ 高校/ 设立/ 数据/ 科学/ 和/ 数据/ 工程/ 相关/ 专业/ ，/ 重点/ 培养/ 专业化/ 数据/ 工程师/ 等/ 大/ 数据/ 专业人才/ 。/ \n",
      "/ 鼓励/ 采取/ 跨校/ 联合/ 培养/ 等/ 方式/ 开展/ 跨学科/ 大/ 数据/ 综合型/ 人才培养/ ，/ 大力/ 培养/ 具有/ 统计分析/ 、/ 计算机技术/ 、/ 经济/ 管理/ 等/ 多/ 学科知识/ 的/ 跨界/ 复合型/ 人才/ 。/ 鼓励/ 高等院校/ 、/ 职业院校/ 和/ 企业/ 合作/ ，/ 加强/ 职业技能/ 人才/ 实践/ 培养/ ，/ 积极/ 培育/ 大/ 数据/ 技术/ 和/ 应用/ 创新型/ 人才/ 。\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(word_text, cut_all=False)\n",
    "\n",
    "## 精确模式\n",
    "\n",
    "print (u\"精确模式：   \\n\")\n",
    "print (\"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果上来看，jieba将`大数据`这个名词，分割为`大`和`数据`两个独立的词。这显示是不符合现实规则的。在jiebe中，我们科通过调整词频，让模型可以将`大数据`整体识别出类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人为调整词频：   \n",
      "\n",
      "加强/ 专业/ 人才培养/ 。/ 创新/ 人才培养/ 模式/ ，/ 建立健全/ 多层次/ 、/ 多/ 类型/ 的/ 大数据/ 人才培养/ 体系/ 。/ 鼓励/ 高校/ 设立/ 数据/ 科学/ 和/ 数据/ 工程/ 相关/ 专业/ ，/ 重点/ 培养/ 专业化/ 数据/ 工程师/ 等/ 大数据/ 专业人才/ 。/ \n",
      "/ 鼓励/ 采取/ 跨校/ 联合/ 培养/ 等/ 方式/ 开展/ 跨学科/ 大数据/ 综合型/ 人才培养/ ，/ 大力/ 培养/ 具有/ 统计分析/ 、/ 计算机技术/ 、/ 经济/ 管理/ 等/ 多/ 学科知识/ 的/ 跨界/ 复合型/ 人才/ 。/ 鼓励/ 高等院校/ 、/ 职业院校/ 和/ 企业/ 合作/ ，/ 加强/ 职业技能/ 人才/ 实践/ 培养/ ，/ 积极/ 培育/ 大数据/ 技术/ 和/ 应用/ 创新型/ 人才/ 。\n"
     ]
    }
   ],
   "source": [
    "jieba.suggest_freq(u'大数据', True)\n",
    "\n",
    "seg_list = jieba.cut(word_text, cut_all=False)\n",
    "\n",
    "## 认为干预\n",
    "\n",
    "print (u\"人为调整词频：   \\n\")\n",
    "print (\"/ \".join(seg_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 <a id=3.2></a>Genius\n",
    "\n",
    "在Genius中，`seg_text()`负责具体的中文分词过程，该函数有多种参数可供选择：\n",
    "\n",
    "|参数|说明|\n",
    "|:---:|:---:|\n",
    "|use_combine|使用字典进行词合并|\n",
    "|use_pinyin|对拼音进行分词处理|\n",
    "|use_tagging|进行词性标注|\n",
    "|use_break|对分词结构打断处理|\n",
    "\n",
    "在这里，我们需要注意的是输入参数的中文文本必须要是`unicode`编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们举例说明。使用的中文文本为：\n",
    "\n",
    ">鼓励高校设立数据科学和数据工程相关专业，重点培养专业化数据工程师等大数据专业人才。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "常见的词性对照表为：\n",
    "\n",
    "|词性缩写|说明|\n",
    "|:---:|:---:|\n",
    "|v|动词|\n",
    "|j|简称略语|\n",
    "|n|名词|\n",
    "|w|标点符号|\n",
    "|d|副词|\n",
    "|a|形容词|\n",
    "|c|连词|\n",
    "|e|叹词|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = u'''鼓励高校设立数据科学和数据工程相关专业，重点培养专业化数据工程师等大数据专业人才。'''\n",
    "\n",
    "seg_list = genius.seg_text(sample_text,\n",
    "                          use_combine = True,\n",
    "                          use_break = False,\n",
    "                          use_tagging = True,\n",
    "                          use_pinyin_segment = False)\n",
    "\n",
    "print ('\\n'.join(['%s\\t%s' % (word.text, word.tagging) for word in seg_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把参数`use_pinyin_segment`的取值设为True， `use_tagging`的取值设为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = genius.seg_text(sample_text,\n",
    "                          use_combine = True,\n",
    "                          use_break = False,\n",
    "                          use_tagging = False,\n",
    "                          use_pinyin_segment = True)\n",
    "\n",
    "print ('\\n'.join([word.text for word in seg_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解参数的基本设置之后，我们对`word_text`进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = genius.seg_text(word_text,\n",
    "                           use_combine = True,\n",
    "                           use_break = False,\n",
    "                           use_tagging = False,\n",
    "                           use_pinyin_segment = False)\n",
    "\n",
    "print (u'使用CRF模型进行中文分词： \\n')\n",
    "\n",
    "print ('/ '.join([word.text for word in seg_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而`seg_keywords()`函数则是Genius库中的另外一种分词函数，分词的结果保留了歧义的分词组合，类似于jieba分词中`cut()`函数的`全模式`。该分词函数的分词结果多适用于搜索引擎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = genius.seg_keywords(sample_text,\n",
    "                               use_break = True,\n",
    "                               use_tagging = True,\n",
    "                               use_pinyin_segment = False,\n",
    "                               use_combine = False)\n",
    "\n",
    "\n",
    "print ('\\n'.join(['%s\\t%s' % (word.text, word.tagging) for word in seg_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从输出结果上来看，`大数据`给出了多种可能的分词组合。然后，我们使用`word_text`文本，查看分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = genius.seg_keywords(word_text,\n",
    "                               use_break = True,\n",
    "                               use_tagging = False,\n",
    "                               use_pinyin_segment = False,\n",
    "                               use_combine = False)\n",
    "\n",
    "\n",
    "print ('\\ '.join([word.text for word in seg_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，Genius还提供了从文本中提取关键词的函数`extract_tag()`函数。保持参数的默认值不变，我们尝试从`word_text`中提取关键词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = genius.extract_tag(word_text)\n",
    "print('\\ '.join(tag_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
